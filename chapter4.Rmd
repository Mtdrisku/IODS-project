# Week 4: Clustering and classification

```{r}
date()
```

This week we will use the data set *Boston* provided in the R package *MASS*. The R documentation file for the data can be found [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html). 

```{r}
# load the data 
library(MASS)

# dimensions
dim(Boston) # 506 observations by 14 variables

# structure
str(Boston) # a data.frame containing numerical data

# two of the variables are integers, let's check their unique values
unique(Boston$chas)
unique(Boston$rad)
```

## 1) Overview of the data
```{r fig.height= 15, fig.width= 15}
# load libraries
library(ggplot2)
library(GGally)

# graphical overview of the data
ggpairs(Boston, upper = list(continuous = wrap("cor", size=4)))

# summaries
summary(Boston)
```

Most of the variables are continuous data, except *chas* and *rad* which are integers. The variables have different distributions and only *rm* shows normal distribution. Variables *indus* and *tax* show bimodal distribution, with most of the values falling in the low or the high end of the scale, and less in the middle. Variables *crim*, *zn*,  *nox*, *dis*, *lstat* and *medv* are more or less left-skewed whereas variables *age*, *pratio* and *black* are right-skewed. There is strong positive correlation between many of the variables, as between *rm* and *medv*, and between *nox* and *age*, and strong negative correlation between as between *rm* and *lstat*, and between *lstat* and *medv*.


## 2) Modifing the data set

```{r}
# standardize the data
boston_scaled <- as.data.frame(scale(Boston))

summary(boston_scaled)
```

The data was standardized by scaling the variables, that is, the column means were subtracted from the corresponding columns and the difference divided by standard deviation. Now the values span between negative and positive, and the difference between the minimum and the maximum value is not as great. Next we will make variable *crim* into a categorical variable and divide the data set into a training set and a test set.

```{r}
# load dplyr
library(dplyr)

# create categories out of the numerical vector and use the quantiles as break points
bins <- quantile(boston_scaled$crim)

# create categorical variable 'crime' out of 'bins'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = T)

# replace 'crim' with the categorical variable 'crime'
boston_scaled <- dplyr::select(boston_scaled, -crim) # remove 'crim'
boston_scaled <- data.frame(boston_scaled, crime) # add 'crime'

names(boston_scaled) # check the variable names again

# create training data set using 80% of the data
n <- nrow(boston_scaled) # number of rows in the data set
n80 <- sample(n, size = n * 0.8) # randomly choose 80% of the rows
train <- boston_scaled[n80,] # training set

# create test data set
test <- boston_scaled[-n80,] 
```

## 3) Linear discriminant analysis 

We will fit a linear discriminant analysis (LDA) on the train set and use *crime* as the target variable. All the other variables are predictor variables in the analysis. The *crime* categories will be predicted from the test data set using the LDA model. 

```{r fig.height=10, fig.width=10}
# linear discriminant analysis fit
lda.fit <- lda(crime ~ ., data = train)

# draw a biplot
# function for the biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# change the 'crime' classes to numeric
classes <- as.numeric(train$crime)

# draw the plot
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1) # add the arrows

# save the 'crime' categories from the test set and remove the variable from the set
crime_categs <- test$crime # save the crime categories into a new object
test <- dplyr::select(test, -crime) # remove variable crime 

# predict the crime categories in the test set
lda.pred <- predict(lda.fit, newdata = test)

# the results
table(correct = crime_categs, predicted = lda.pred$class)
```

The LDA model fitted using 80% of the original data set performs quite well in predicting the categories of the *crime* variable. The lowest and the highest crime rates best predicted, but the two categories between these rates are not as easily correctly predicted by the LDA model. 


## 4) Clustering 

```{r fig.height=15, fig.width=15, setup, warning=FALSE}
# reload the Boston data set
data('Boston')

# scale the data
boston_scaled_2 <- scale(Boston)

# calculate distance between the observations
dist_eu <- dist(boston_scaled_2, method = "euclidean")

# k-means clustering
library(factoextra)
set.seed(12345)
fviz_nbclust(boston_scaled_2, kmeans, method = "wss") # find the optimal amount of clusters
km <- kmeans(Boston, centers = 3) # optimal number of clusters is 3
km # print the output

# plot the clusters in a pair plot
ggpairs(Boston, mapping = aes(col = as.factor(km$cluster), alpha = 0.5),
        upper = list(continuous = wrap("cor", size = 4)))
```

Cluster 3 (blue points) seems to divert from the other two clusters (green and red points).